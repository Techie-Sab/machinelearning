{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hugging_Face_QNA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP71S19hbOdhEfyGf855ONr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Techie-Sab/machinelearning/blob/main/Hugging_Face_QNA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "xlEogv9DmOke",
        "outputId": "6c6feb14-35e8-4a6b-b503-ebd22f18afc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-01a0301fcd81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/notebooks'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab Notebooks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/My Drive/Colab Notebooks' -> '/content/notebooks'"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M6YhKWQmWWN",
        "outputId": "4cfdbd07-559a-4c3c-9e94-6026a77cf5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 11.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "XRjWTYY3nPwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNA():\n",
        "    def __init__(self):\n",
        "\n",
        "      try:\n",
        "\n",
        "        self.tokenizer=AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "        self.model= AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "        self.qa=pipeline('question-answering', model=self.model, tokenizer=self.tokenizer)\n",
        "      except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "    def cleaning_text(self,text):\n",
        "      text = text.lstrip()\n",
        "      text = text.rstrip()\n",
        "      self.temp = re.sub(\"\\n+\",\" \",text)\n",
        "    \n",
        "      return self.temp\n",
        "\n",
        "    def save_model(self):\n",
        "      #saving the tokens\n",
        "      self.tokenizer.save_pretrained('/content/drive/MyDrive/tokenizer/')\n",
        "      #saving the model\n",
        "      self.model.save_pretrained('/content/drive/MyDrive/Q&A/')\n",
        "\n",
        "\n",
        "    def predict_ans(self,context,ques):\n",
        "      ans = []\n",
        "\n",
        "      try:\n",
        "        if str(type(ques)) == \"<class 'list'>\":\n",
        "\n",
        "          print(\"Inside list\")\n",
        "\n",
        "          for i in ques:\n",
        "            req_ques = {\n",
        "                'question':i,\n",
        "                'context':context\n",
        "            }\n",
        "            print(req_ques)\n",
        "            sol = { 'ques':i, 'ans':self.qa(req_ques)['answer'] }\n",
        "\n",
        "            ans.append(sol)\n",
        "\n",
        "        else:\n",
        "          req_ques = {\n",
        "                'question':ques,\n",
        "                'context':context\n",
        "            }\n",
        "          sol = {\n",
        "              'ques':ques,\n",
        "              'ans':self.qa(req_ques)['answer']\n",
        "          }\n",
        "          ans.append(sol)\n",
        "        return ans      \n",
        "\n",
        "      except:\n",
        "        pass\n",
        "    "
      ],
      "metadata": {
        "id": "R7JpLNgEoVRO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = QNA()"
      ],
      "metadata": {
        "id": "eb1oYq4btoru"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = [[\"Of the two countries that produce soybeans, which country is clearing rain forest in order to increase production?\"]]\n",
        "            \n",
        "context = [\"\"\"\n",
        "Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi), \n",
        "with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested \n",
        "since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States. \n",
        "New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less \n",
        "precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no \n",
        "economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.\"\"\"]"
      ],
      "metadata": {
        "id": "ToPoOMX-tsT9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleansed = []\n",
        "for each in context:\n",
        "  cleansed.append(model.cleaning_text(each))"
      ],
      "metadata": {
        "id": "1t0JjYYzvJ0l"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans = []\n",
        "for i,j in zip(cleansed,question):\n",
        "  ans.append(model.predict_ans(i,j))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCHBberkuAvQ",
        "outputId": "30010c6a-6a11-4328-d2f8-ac4c00006886"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inside list\n",
            "{'question': 'Of the two countries that produce soybeans, which country is clearing rain forest in order to increase production?', 'context': 'Between 1991 and 2000, the total area of forest lost in the Amazon rose from 415,000 to 587,000 square kilometres (160,000 to 227,000 sq mi),  with most of the lost forest becoming pasture for cattle. Seventy percent of formerly forested land in the Amazon, and 91% of land deforested  since 1970, is used for livestock pasture. Currently, Brazil is the second-largest global producer of soybeans after the United States.  New research however, conducted by Leydimere Oliveira et al., has shown that the more rainforest is logged in the Amazon, the less  precipitation reaches the area and so the lower the yield per hectare becomes. So despite the popular perception, there has been no  economical advantage for Brazil from logging rainforest zones and converting these to pastoral fields.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Question: {}\".format(ans[0][0]['ques']))\n",
        "print(\"Answer: {}\".format(ans[0][0]['ans']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q788_c0cwjJc",
        "outputId": "24b53191-c488-4850-ff8b-c598d8060d2d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Of the two countries that produce soybeans, which country is clearing rain forest in order to increase production?\n",
            "Answer: Brazil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_model()"
      ],
      "metadata": {
        "id": "1A2ypOnWuIRk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "08sYLc16xPiz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}